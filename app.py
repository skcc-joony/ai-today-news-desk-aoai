import os
import requests
import streamlit as st
import numpy as np
from bs4 import BeautifulSoup
import openai
import faiss
from dotenv import load_dotenv

# --- í™˜ê²½ë³€ìˆ˜ ë¡œë”© ---
load_dotenv()
AZURE_OPENAI_ENDPOINT = os.environ["AZURE_OPENAI_ENDPOINT"]
AZURE_OPENAI_API_KEY = os.environ["AZURE_OPENAI_API_KEY"]
AZURE_OPENAI_API_VERSION = os.environ["AZURE_OPENAI_API_VERSION"]
EMBEDDING_DEPLOY = os.environ["EMBEDDING_DEPLOY"]
CHAT_DEPLOY = os.environ["CHAT_DEPLOY"]

openai.api_type = "azure"
openai.api_key = AZURE_OPENAI_API_KEY
openai.api_base = AZURE_OPENAI_ENDPOINT
openai.api_version = AZURE_OPENAI_API_VERSION

NEWS_FILE = "data/news_today.txt"
EMBEDDINGS_FILE = "data/faiss.index"
NEWS_JSON = "data/news_list.npy"

def ensure_data_dir():
    if not os.path.exists("data"):
        os.makedirs("data")

# --- 1. Bloomberg Asia ë‰´ìŠ¤ í¬ë¡¤ë§ ---
def crawl_bloomberg_asia():
    url = "https://www.bloomberg.com/asia"
    resp = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, verify=False)
    soup = BeautifulSoup(resp.text, "html.parser")
    results = []

    # 1. Top headline ë°©ì‹ (ìµœê·¼ ê¸°ì¤€)
    for tag in soup.select('a[data-tracking-context="section_headline"]'):
        title = tag.get_text(strip=True)
        link = tag.get('href')
        if link and not link.startswith('http'):
            link = "https://www.bloomberg.com" + link
        if title and link and link.startswith("https://www.bloomberg.com"):
            results.append({"title": title, "link": link})

    # 2. í˜¹ì‹œ ì•ˆ ë‚˜ì˜¤ë©´ backupìœ¼ë¡œ /news/articles/ ê²½ë¡œ ëŒ€ìƒ
    if not results:
        for tag in soup.find_all("a"):
            title = tag.get_text(strip=True)
            link = tag.get("href")
            if title and link and "/news/articles/" in link:
                if not link.startswith('http'):
                    link = "https://www.bloomberg.com" + link
                results.append({"title": title, "link": link})

    # 3. 30ê°œ
    return results[:30]


# --- 2. ì„ë² ë”© ìƒì„± (Azure OpenAI) ---
def get_azure_embedding(texts):
    results = []
    for text in texts:
        response = openai.embeddings.create(
            input=text,
            model=EMBEDDING_DEPLOY  # Azure ë°°í¬ì´ë¦„!
        )
        vec = response.data[0].embedding
        results.append(np.array(vec, dtype=np.float32))
    return results


# --- 3. FAISS ë²¡í„°DB ìƒì„± ---
def build_faiss_db(news_list):
    texts = [n["title"] for n in news_list]
    embeddings = get_azure_embedding(texts)
    dim = len(embeddings[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.vstack(embeddings))
    return index, embeddings

def save_faiss_db(index, news_list):
    faiss.write_index(index, EMBEDDINGS_FILE)
    np.save(NEWS_JSON, np.array(news_list, dtype=object))

def load_faiss_db():
    if not os.path.exists(EMBEDDINGS_FILE) or not os.path.exists(NEWS_JSON):
        return None, []
    index = faiss.read_index(EMBEDDINGS_FILE)
    news_list = np.load(NEWS_JSON, allow_pickle=True).tolist()
    return index, news_list

# --- 4. ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ---
def search_news(query, index, news_list, top_k=3):
    q_emb = get_azure_embedding([query])[0]
    D, I = index.search(np.array([q_emb]), top_k)
    results = [news_list[i] for i in I[0]]
    return results

# --- 5. GPT-4oë¡œ ë‹µë³€ ìƒì„± ---
def ask_azure_openai(context, question):
    messages = [
        {"role": "system", "content": "ë„ˆëŠ” ë¸”ë£¸ë²„ê·¸ ì•„ì‹œì•„ ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ë¥¼ ìš”ì•½/ë¶„ì„í•´ì£¼ëŠ” AI ë‰´ìŠ¤ ë¹„ì„œì•¼."},
        {"role": "user", "content": f"ê´€ë ¨ ë‰´ìŠ¤ë“¤:\n{context}\n\nì§ˆë¬¸: {question}"}
    ]
    response = openai.chat.completions.create(
        model=CHAT_DEPLOY,    # Azure ë°°í¬ì´ë¦„!
        messages=messages,
        temperature=0.2,
        max_tokens=2000
    )
    return response.choices[0].message.content


# --- 6. Streamlit UI ---
st.set_page_config(page_title="Bloomberg Asia ë‰´ìŠ¤ AI", page_icon="ğŸ“°", layout="wide")
ensure_data_dir()

st.title("ğŸ“° ì˜¤ëŠ˜ì˜ Bloomberg Asia ë‰´ìŠ¤ AI")
st.markdown("Bloomberg Asiaì˜ ì˜¤ëŠ˜ ë‰´ìŠ¤, í•«ì´ìŠˆ, í‚¤ì›Œë“œë¥¼ AIë¡œ ìš”ì•½/ë¶„ì„í•©ë‹ˆë‹¤.")

if st.button("ì˜¤ëŠ˜ ë‰´ìŠ¤ ìµœì‹ í™” (í¬ë¡¤ë§/ì„ë² ë”©)"):
    with st.spinner("ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘..."):
        news = crawl_bloomberg_asia()
    if not news:
        st.warning("ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        with st.spinner("ì„ë² ë”©/FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘..."):
            index, embeddings = build_faiss_db(news)
            save_faiss_db(index, news)
        st.success("ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ì„ë² ë”© ì™„ë£Œ!")

index, news_list = load_faiss_db()
if not news_list:
    st.info("ë¨¼ì € 'ì˜¤ëŠ˜ ë‰´ìŠ¤ ìµœì‹ í™”' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

question = st.text_input("ë‰´ìŠ¤ ë¶„ì„ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì˜¤ëŠ˜ì˜ í•«ì´ìŠˆ, ì£¼ìš” í‚¤ì›Œë“œ, ìš”ì•½ ë“±)")
if st.button("AI ë‰´ìŠ¤ ë¶„ì„ ìš”ì²­") and question and index:
    sims = search_news(question, index, news_list)
    context = "\n".join([f"- {n['title']} ({n['link']})" for n in sims])
    with st.spinner("GPT-4oê°€ ë‹µë³€ ìƒì„± ì¤‘..."):
        answer = ask_azure_openai(context, question)
    st.markdown("#### ğŸ¤– AI ë‰´ìŠ¤ ë‹µë³€")
    st.success(answer)
    st.markdown("##### ğŸ” ì°¸ê³  ë‰´ìŠ¤")
    for n in sims:
        st.info(f"{n['title']}\n{n['link']}")

st.caption("Â© 2025 AI Bootcamp. Powered by Azure OpenAI & Bloomberg")
